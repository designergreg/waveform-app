<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Waveform Actionbar</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
<style>
  html, body {
    margin: 0;
    padding: 0;
    height: 100%;
    overflow: hidden;
    -webkit-user-select: none;
    user-select: none;
    -webkit-touch-callout: none;
    touch-action: none;
  }

  /* Fullscreen video background */
  #bgVideo {
    position: fixed;
    top: 0;
    left: 0;
    width: 100vw;
    height: 100vh;
    object-fit: cover;
    z-index: -2;
  }

  /* Actionbar container */
  .actionbar {
    position: fixed;
    bottom: 20px;
    left: 50%;
    transform: translateX(-50%);
    width: 343px;
    height: 60px;
    border-radius: 30px;
    background: rgba(255, 0, 0, 0.25);
    display: flex;
    align-items: center;
    justify-content: space-between;
    padding: 0 6px;
    overflow: hidden;
    z-index: 0;
  }

  /* Waveform canvas fills the actionbar */
  #wave {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    z-index: 0;
  }

  /* Icon buttons */
  .icon-button {
    width: 48px;
    height: 48px;
    border-radius: 24px;
    background: rgba(0,0,0,0.7);
    backdrop-filter: blur(4px);
    display: flex;
    align-items: center;
    justify-content: center;
    z-index: 1;
    cursor: pointer;
    transition: background 0.2s, opacity 0.2s;
  }

  .icon-button img {
    width: 24px;
    height: 24px;
    object-fit: contain;
    filter: brightness(0) invert(1); /* white icon */
  }

  /* Hover */
  .icon-button:hover {
    background: #1F4C7D;
  }
</style>
</head>
<body>

<video id="bgVideo" src="therapist_silent_30.mp4" autoplay loop muted playsinline></video>

<div class="actionbar">
  <!-- waveform background -->
  <canvas id="wave"></canvas>

  <!-- left pause button -->
  <div class="icon-button">
    <img src="icons/linear/pause.svg" alt="Pause">
  </div>

  <!-- right more button -->
  <div class="icon-button">
    <img src="icons/linear/more.svg" alt="More">
  </div>
</div>

<script>
/* ---------- Canvas & resize ---------- */
const canvas = document.getElementById("wave");
const ctx = canvas.getContext("2d");
function resize() {
  canvas.width = canvas.clientWidth;
  canvas.height = canvas.clientHeight;
}
resize();
window.addEventListener("resize", resize);

/* ---------- Mobile detection ---------- */
const isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);

/* ---------- Audio / Mic pipeline (always running after init) ---------- */
let micInitialized = false;
let smoothedRMS = 0;
let targetAmplitude = 0;

// Tunable constants (different on mobile)
const BASE_AMPLITUDE = isMobile ? 20 : 12;
const MIN_WAVE = isMobile ? 15 : 10;
const RESPONSIVE_BOOST = isMobile ? 180 : 120;
const VOICE_Y_SHIFT = isMobile ? 30 : 20;

// RMS scaling constants (we amplify with GainNode and slightly increase multiplier)
const RMS_MULTIPLIER = isMobile ? 6 : 5;   // we rely on GainNode primarily; keep multiplier modest
const RMS_EXPONENT = isMobile ? 1.3 : 1.2;

// Will be populated by setupMic
let audioCtx = null;
let analyser = null;
let buffer = null;

async function setupMicPipeline() {
  if (micInitialized) return;
  micInitialized = true;

  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      audio: { echoCancellation: false, noiseSuppression: false, autoGainControl: false }
    });

    // create AudioContext
    audioCtx = new (window.AudioContext || window.webkitAudioContext)();

    // create nodes: source -> gain -> analyser
    const source = audioCtx.createMediaStreamSource(stream);

    // Gain to boost mobile signals (non-destructive — amplifies for analyser)
    const gainNode = audioCtx.createGain();
    // On mobile use stronger gain; on desktop keep 1
    gainNode.gain.value = isMobile ? 6.0 : 1.0; // tweak if needed

    analyser = audioCtx.createAnalyser();
    analyser.fftSize = 512; // reasonable tradeoff
    buffer = new Uint8Array(analyser.frequencyBinCount);

    // Connect: source -> gain -> analyser -> destination (no output to speakers)
    source.connect(gainNode);
    gainNode.connect(analyser);
    // DO NOT connect analyser to destination (we don't want it to play back)
    // but you can optionally connect gainNode -> audioCtx.destination if you want monitoring.

    // Start the read loop
    requestAnimationFrame(audioLoop);
  } catch (err) {
    console.error("setupMicPipeline error:", err);
    micInitialized = false;
  }
}

// Audio processing loop (always running via requestAnimationFrame)
function audioLoop() {
  if (!analyser || !buffer) {
    requestAnimationFrame(audioLoop);
    return;
  }

  analyser.getByteTimeDomainData(buffer);

  // compute RMS
  let sum = 0;
  for (let i = 0; i < buffer.length; i++) {
    const v = (buffer[i] - 128) / 128;
    sum += v * v;
  }
  let rawRMS = Math.sqrt(sum / buffer.length);

  if (rawRMS < 0.005) rawRMS = 0;

  // scale (multiplier + exponent) — gain node already amplified the signal, so keep multiplier moderate
  const scaledRMS = Math.pow(rawRMS * RMS_MULTIPLIER, RMS_EXPONENT);

  // attack/release smoothing (envelope)
  const ATTACK = 0.8;
  const RELEASE = 0.05;
  if (scaledRMS > smoothedRMS) {
    smoothedRMS = smoothedRMS * (1 - ATTACK) + scaledRMS * ATTACK;
  } else {
    smoothedRMS = smoothedRMS * (1 - RELEASE) + scaledRMS * RELEASE;
  }

  // compute visual amplitude used by rendering
  targetAmplitude = MIN_WAVE + BASE_AMPLITUDE + smoothedRMS * RESPONSIVE_BOOST;

  requestAnimationFrame(audioLoop);
}

/* Start mic on first user gesture (required on mobile). On desktop this will run immediately if you trigger a click or press. */
function initMicOnFirstGesture() {
  // If already initialized, nothing to do
  if (micInitialized) return;

  // Kick off pipeline and resume audio context if needed
  setupMicPipeline().then(() => {
    if (audioCtx && audioCtx.state === 'suspended') {
      audioCtx.resume().catch(() => {});
    }
  }).catch(() => {});
  // remove listeners after first gesture
  document.body.removeEventListener('touchstart', initMicOnFirstGesture);
  document.body.removeEventListener('click', initMicOnFirstGesture);
}
document.body.addEventListener('touchstart', initMicOnFirstGesture, { once: true, passive: true });
document.body.addEventListener('click', initMicOnFirstGesture, { once: true, passive: true });

/* ---------- Perlin noise for smooth hills ---------- */
const noise = {
  grad: Array.from({length:256}, () => Math.random()*2-1),
  fade: t => t*t*t*(t*(t*6-15)+10),
  lerp: (a,b,t) => a + (b-a)*t,
  get(x){
    const X = Math.floor(x) & 255;
    const t = x - Math.floor(x);
    const g1 = this.grad[X], g2 = this.grad[X+1];
    return this.lerp(g1*t, g2*(t-1), this.fade(t));
  }
};

/* ---------- Wave rendering ---------- */
const speed = 0.004;
const layers = [
  { color: '#5A8696', alpha: 0.6, xShift: 0 },
  { color: '#CD568A', alpha: 0.6, xShift: 80 },
  { color: '#9E6FA8', alpha: 0.6, xShift: 160 },
  { color: '#0D4CAC', alpha: 0.7, xShift: 240 }
];
let t = 0;
const peaks = 2;

/* fade control + UI */
let isTalking = false;
let fadeFactor = 1; // since always-on, waveform visible => keep fadeFactor at 1 by default
const FADE_SPEED = 0.05;

/* Input listeners (keeps isTalking true while user holds) */
window.addEventListener("keydown", e => { if (e.code === "Space") isTalking = true; });
window.addEventListener("keyup", e => { if (e.code === "Space") isTalking = false; });
["touchstart","touchend","touchcancel"].forEach(evt=>{
  document.body.addEventListener(evt, e => {
    // avoid interfering with scrolling; only prevent default when necessary
    if (evt === "touchstart") { e.preventDefault(); isTalking = true; }
    else { e.preventDefault(); isTalking = false; }
  }, { passive: false });
});

/* render one layer */
function drawWave(layer, width, height, alphaMultiplier = 1) {
  ctx.beginPath();
  ctx.moveTo(0, height);
  const noiseScale = peaks / width;
  const dynamicOffset = smoothedRMS * VOICE_Y_SHIFT;
  const baseY = height * 0.75 - dynamicOffset;

  for (let x = 0; x < width; x++) {
    const n = noise.get((x + layer.xShift) * noiseScale + t * speed);
    const normalized = n * 0.5;
    const y = baseY - normalized * targetAmplitude;
    ctx.lineTo(x, y);
  }

  ctx.lineTo(width, height);
  ctx.closePath();

  const r = parseInt(layer.color.slice(1,3),16);
  const g = parseInt(layer.color.slice(3,5),16);
  const b = parseInt(layer.color.slice(5,7),16);
  ctx.fillStyle = `rgba(${r}, ${g}, ${b}, ${layer.alpha * alphaMultiplier})`;
  ctx.fill();
}

/* show/hide buttons based on waveform visibility */
const buttons = document.querySelectorAll(".icon-button");

/* main draw loop */
function draw() {
  ctx.clearRect(0, 0, canvas.width, canvas.height);

  // keep waveform visible (fadeFactor should remain 1); but preserve the original fade behavior if you want.
  if (isTalking) {
    fadeFactor += FADE_SPEED;
    if (fadeFactor > 1) fadeFactor = 1;
  } else {
    fadeFactor -= FADE_SPEED;
    if (fadeFactor < 0) fadeFactor = 0;
  }

  // Because we want always-running pipeline and visible waveform, ensure fadeFactor stays >0.
  // If you truly want always-fully-visible set fadeFactor = 1;
  // fadeFactor = 1;

  // show/hide buttons (when waveform visible, hide)
  buttons.forEach(btn => {
    btn.style.display = fadeFactor > 0 ? "none" : "flex";
  });

  if (fadeFactor > 0) {
    layers.forEach(layer => drawWave(layer, canvas.width, canvas.height, fadeFactor));
  }

  t += 1;
  requestAnimationFrame(draw);
}

draw();
</script>

</body>
</html>